{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for sentence classification\n",
    "- [Reference code](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb) by bentrevett\n",
    "- Dataset [download](https://github.com/yoonkim/CNN_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:48.564842Z",
     "start_time": "2021-01-19T04:38:48.043775Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:49.559983Z",
     "start_time": "2021-01-19T04:38:49.547014Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed= 1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:50.734789Z",
     "start_time": "2021-01-19T04:38:50.722837Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 준비 함수 (None -> df)\n",
    "def load_data():\n",
    "    neg_path = r\"D:\\SIMON\\대학원\\9_스터디\\2021_pytorch_study\\1월_CNN\\1_reference_code\\rt-polarity.neg.txt\"\n",
    "    neg_df = pd.read_table(neg_path, header=None, names=['X'], encoding='latin') #'ISO-8859-1' 의 alias\n",
    "    neg_df['y'] = [0] * len(neg_df)\n",
    "\n",
    "    pos_path = r\"D:\\SIMON\\대학원\\9_스터디\\2021_pytorch_study\\1월_CNN\\1_reference_code\\rt-polarity.pos.txt\"\n",
    "    pos_df = pd.read_table(pos_path, header=None, names=['X'], encoding='latin') #'ISO-8859-1' 의 alias\n",
    "    pos_df['y'] = [1] * len(pos_df)\n",
    "\n",
    "    data = pd.concat([neg_df, pos_df], axis=0)\n",
    "    data.reset_index(inplace=True)\n",
    "    print(\"# of Loaded Data: {}\".format(data.shape[0]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:51.217900Z",
     "start_time": "2021-01-19T04:38:51.186822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Loaded Data: 10662\n"
     ]
    }
   ],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:53.196511Z",
     "start_time": "2021-01-19T04:38:52.394687Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:53.227531Z",
     "start_time": "2021-01-19T04:38:53.212547Z"
    }
   },
   "outputs": [],
   "source": [
    "#텍스트 전처리 함수 (str -> lst)\n",
    "def text_preprocessor(sent):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    # text preprocessing\n",
    "    sent_str = sent.lower()\n",
    "    sent_str = re.sub('[^a-z]', ' ', sent_str)  # Remove non-alphabetic strings\n",
    "    sent_str = re.sub('  ', ' ', sent_str).strip()  # Remove double white spaces\n",
    "    sent_lst = [word for word in sent_str.split() if word not in stop] # Remove stopwords\n",
    "    return sent_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:55.668036Z",
     "start_time": "2021-01-19T04:38:53.651268Z"
    }
   },
   "outputs": [],
   "source": [
    "data['X_lst'] = data['X'].map(text_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:55.699271Z",
     "start_time": "2021-01-19T04:38:55.686093Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 분리 함수 (df -> [df,df,df])\n",
    "def data_split(df):\n",
    "    train_df = df.sample(frac=0.8, random_state=123)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    val_df = train_df.sample(frac=0.2, random_state=123)\n",
    "    train_df = train_df.drop(val_df.index)\n",
    "    \n",
    "    print(\"Train_df shape:\", train_df.shape)\n",
    "    print(\"Val_df shape:\", val_df.shape)\n",
    "    print(\"Test_df shape:\", test_df.shape)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:56.922521Z",
     "start_time": "2021-01-19T04:38:56.898550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_df shape: (6824, 4)\n",
      "Val_df shape: (1706, 4)\n",
      "Test_df shape: (2132, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = data_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:58.127817Z",
     "start_time": "2021-01-19T04:38:58.120277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vocab 만들어주는 함수 ([df,df] -> [dict, dict])\n",
    "def buildVocab(train_df, val_df):\n",
    "    '''train과 valid 데이터만 사용 주의'''\n",
    "    trainWords_lst = [w for w_lst in train_df['X_lst'] for w in w_lst]\n",
    "    ValWords_lst = [w for w_lst in val_df['X_lst'] for w in w_lst]\n",
    "    totalWords_lst = trainWords_lst + ValWords_lst\n",
    "\n",
    "    words_lst = ['<pad>', '<unk>'] + sorted(list(set(totalWords_lst)))\n",
    "\n",
    "    itos = {idx : word for idx, word in enumerate(words_lst)}\n",
    "    stoi = {word : idx for idx, word in enumerate(words_lst)}\n",
    "\n",
    "    print('length of word_set:', len(words_lst))\n",
    "    return itos, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:38:59.015067Z",
     "start_time": "2021-01-19T04:38:58.986871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_set: 16278\n"
     ]
    }
   ],
   "source": [
    "itos, stoi = buildVocab(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String to Index (Numericalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:00.118709Z",
     "start_time": "2021-01-19T04:39:00.106611Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문장을 숫자로 표현해주는 함수 (lst -> lst)\n",
    "def token_to_idx(token_lst, stoi):\n",
    "    idx_lst = []\n",
    "    for w in token_lst:\n",
    "        if w in stoi:\n",
    "            idx = stoi[w]\n",
    "        else:\n",
    "            idx = stoi['<unk>']\n",
    "        idx_lst.append(idx)\n",
    "    return idx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:00.974511Z",
     "start_time": "2021-01-19T04:39:00.930295Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['X_idx'] = train_df['X_lst'].map(lambda x: token_to_idx(x, stoi))\n",
    "val_df['X_idx'] = val_df['X_lst'].map(lambda x: token_to_idx(x, stoi))\n",
    "test_df['X_idx'] = test_df['X_lst'].map(lambda x: token_to_idx(x, stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:01.480810Z",
     "start_time": "2021-01-19T04:39:01.474291Z"
    }
   },
   "outputs": [],
   "source": [
    "# zero padding을 통해서 시퀀스의 길이를 맞춰주는 함수 ([lst, int, dict] -> lst)\n",
    "def zero_padding(idx_lst, max_len, stoi):\n",
    "    pad_idx = stoi['<pad>']\n",
    "    unk_idx = stoi['<unk>']\n",
    "    \n",
    "    idx_lst = idx_lst[:max_len]\n",
    "    \n",
    "    if len(idx_lst) == max_len:\n",
    "        return idx_lst\n",
    "    else:\n",
    "        padding_len = max_len - len(idx_lst)\n",
    "        padding_list = [pad_idx] * padding_len\n",
    "        idx_lst = idx_lst + padding_list\n",
    "        return idx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:02.531168Z",
     "start_time": "2021-01-19T04:39:02.506286Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['X_pad'] = train_df['X_idx'].map(lambda x: zero_padding(x, 10, stoi))\n",
    "val_df['X_pad'] = val_df['X_idx'].map(lambda x: zero_padding(x, 10, stoi))\n",
    "test_df['X_pad'] = test_df['X_idx'].map(lambda x: zero_padding(x, 10, stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:46:07.915066Z",
     "start_time": "2021-01-19T04:46:07.899000Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x # np.array 말고 list로도 가능?\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_x = torch.LongTensor(self.x[idx])\n",
    "        target_y = torch.FloatTensor([self.y[idx]]) # 왜 FloatTensor?,torch.FloatTensor([]) 이렇게 넣어줘야하는듯.\n",
    "        return input_x, target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:46:08.132915Z",
     "start_time": "2021-01-19T04:46:08.122994Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기존 idx를 제거하기 위해서 list() 사용\n",
    "trainset = CustomDataset(list(train_df['X_pad']), list(train_df['y']))\n",
    "validset = CustomDataset(list(val_df['X_pad']), list(val_df['y']))\n",
    "testset = CustomDataset(list(test_df['X_pad']), list(test_df['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:46:08.432818Z",
     "start_time": "2021-01-19T04:46:08.418873Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "- In pytorch, CNN wants the batch dimension first!\n",
    "- in_channels: # of channels / out_channels: # of filters / kernel_size: filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:46:09.488195Z",
     "start_time": "2021-01-19T04:46:09.482182Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:46:09.710308Z",
     "start_time": "2021-01-19T04:46:09.707316Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:47:29.824515Z",
     "start_time": "2021-01-19T04:47:29.818549Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze= True, padding_idx= pad_idx)\n",
    "        \n",
    "        # Note List comprehension\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels= 1,  # 1 channel for TEXT\n",
    "                                              out_channels= n_filters,\n",
    "                                              kernel_size = (fs, embedding_dim))\n",
    "                                    for fs in filter_sizes])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim) # Because of simply \"CONCATENATE\"!\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text = [batch_size, sent_len]\n",
    "        embedded = self.embedding(text) \n",
    "        # embedded = [batch_size, sent_len, embed_dim]\n",
    "        embedded = embedded.unsqueeze(1) # Insert 1 dimension to represent # of channels like images\n",
    "        \n",
    "        # embedded = [batch_size, 1, sent_len, embed_dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # WHY does Squeeze need here?\n",
    "        \n",
    "        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        \n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embedding\n",
    "- Load the pretrain Word2Vec model from Google [(Download HERE)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) \n",
    "- It might take time since it  contains 300-dimensional vectors for 3 million words and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:18.274097Z",
     "start_time": "2021-01-19T04:39:18.130274Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:39:33.453488Z",
     "start_time": "2021-01-19T04:39:20.123269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained_embeddings\n",
    "path = r\"C:\\Users\\Simon\\ongoing_projects\\SSRC_collaboration\\source\\GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "# Customize pretrained_embeddings \n",
    "pretrained_embeddings = np.zeros((len(stoi), 300)) # <pad>, <unk>은 여기서는 제외시켜야 하나??\n",
    "\n",
    "# unk_lst = []\n",
    "for i, w in enumerate(list(stoi.keys())):\n",
    "    try:\n",
    "        tmp_vec = word_vectors[w]\n",
    "        pretrained_embeddings[i] = tmp_vec\n",
    "    except KeyError:\n",
    "        pass\n",
    "#         unk_lst.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:33:34.919687Z",
     "start_time": "2021-01-19T04:33:34.905140Z"
    }
   },
   "outputs": [],
   "source": [
    "# unk_lst.remove(0) # <pad>\n",
    "# unk_lst.remove(1) # <unk>\n",
    "# pretrained_embeddings = np.delete(pretrained_embeddings, unk_lst, axis=0) # <unk>이 있기때문에 없는 단어들 제거????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:47:32.730944Z",
     "start_time": "2021-01-19T04:47:32.722405Z"
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# INPUT_DIM = len(stoi)\n",
    "PRETRAINED_EMB = torch.from_numpy(pretrained_embeddings) # torch.tensor로 전환 주의\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:47:54.772533Z",
     "start_time": "2021-01-19T04:47:54.754605Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CNN(PRETRAINED_EMB, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "model = model.float()  # Reference: https://github.com/KimythAnly/AGAIN-VC/issues/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:47:58.887894Z",
     "start_time": "2021-01-19T04:47:58.881887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 360,601 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:47:59.736509Z",
     "start_time": "2021-01-19T04:47:59.729901Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:48:00.812924Z",
     "start_time": "2021-01-19T04:48:00.799267Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:48:40.391330Z",
     "start_time": "2021-01-19T04:48:40.378351Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for x, y in iterator:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(x)\n",
    "#         predictions = model(x).squeeze(1)\n",
    "#         print(\"predictions:\", predictions.shape)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:48:40.609013Z",
     "start_time": "2021-01-19T04:48:40.602032Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            predictions = model(x)\n",
    "#             predictions = model(x).squeeze(1)\n",
    "            loss = criterion(predictions, y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:48:40.983784Z",
     "start_time": "2021-01-19T04:48:40.978798Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:50:24.898423Z",
     "start_time": "2021-01-19T04:50:14.066637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.60%\n",
      "\t Val. Loss: 0.891 |  Val. Acc: 76.09%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.77%\n",
      "\t Val. Loss: 0.916 |  Val. Acc: 75.31%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.78%\n",
      "\t Val. Loss: 0.932 |  Val. Acc: 75.47%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.71%\n",
      "\t Val. Loss: 0.947 |  Val. Acc: 75.59%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.010 | Train Acc: 99.78%\n",
      "\t Val. Loss: 0.952 |  Val. Acc: 75.19%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.71%\n",
      "\t Val. Loss: 0.982 |  Val. Acc: 75.48%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.63%\n",
      "\t Val. Loss: 0.993 |  Val. Acc: 75.30%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.85%\n",
      "\t Val. Loss: 0.992 |  Val. Acc: 75.89%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.008 | Train Acc: 99.82%\n",
      "\t Val. Loss: 0.974 |  Val. Acc: 75.97%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.997 |  Val. Acc: 75.57%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, trainloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, validloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T04:51:58.948189Z",
     "start_time": "2021-01-19T04:51:58.834493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.943 | Test Acc: 72.14%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, testloader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.667,
   "position": {
    "height": "196.667px",
    "left": "921px",
    "right": "20px",
    "top": "85px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
